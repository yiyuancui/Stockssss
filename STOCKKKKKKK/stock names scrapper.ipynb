{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de5c8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "### this file is intended to scrap stock names off of Finviz, with screening filter setted as Volumes over 1 Million,\n",
    "### Price over 1 usd, volatility week over 5%, ATR > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4ec7e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1 of 5\n",
      "Scraping page 2 of 5\n",
      "Scraping page 3 of 5\n",
      "Scraping page 4 of 5\n",
      "Scraping page 5 of 5\n",
      "['AAOI', 'AGQ', 'AKRO', 'ALAB', 'AMC', 'APLS', 'ARQT', 'ARRY', 'AWIN', 'BE', 'BHVN', 'BITX', 'BOIL', 'BYON', 'CABA', 'CADL', 'CENX', 'CLSK', 'COIN', 'CONL', 'DELL', 'DJT', 'DPST', 'DXYZ', 'DYN', 'EDBL', 'ELF', 'ENPH', 'ENVX', 'EXAS', 'FLNC', 'FSLR', 'FWRD', 'GCT', 'GDXU', 'GH', 'GME', 'GTBP', 'HIMS', 'HOOD', 'IBRX', 'INSM', 'IRBT', 'JAGX', 'JANX', 'KOLD', 'LABU', 'LEGN', 'MARA', 'MGNI', 'MGNX', 'MRNA', 'MSTR', 'MULN', 'NRIX', 'NVAX', 'NVDL', 'NXT', 'OKLO', 'OSCR', 'OUST', 'PALI', 'PGY', 'PLCE', 'RDDT', 'RENT', 'RILY', 'RIOT', 'RNA', 'RUN', 'SAGE', 'SG', 'SGBX', 'SMCI', 'SMR', 'SOXL', 'SOXS', 'SPT', 'STOK', 'TNDM', 'TSLT', 'TSLZ', 'UNG', 'VSAT', 'VST', 'WISA', 'WOLF', 'WSM']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_finviz_stocks():\n",
    "    \n",
    "    # this url contains filters applied to the selection\n",
    "    base_url = \"https://finviz.com/screener.ashx?v=111&f=geo_usa,sh_avgvol_o1000,sh_price_o1,ta_averagetruerange_o0.75,ta_volatility_wo5&ft=3\"\n",
    "    \n",
    "    # mimic human interaction to present scipt blocking\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    \n",
    "    # Debug to make sure the url is accessable\n",
    "    response = requests.get(base_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "        return []\n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    total_pages = get_total_pages(soup)\n",
    "    \n",
    "    stocks = []\n",
    "    \n",
    "    # since there are different pages with the stocks in, we interate through each page to get the stock names\n",
    "    \n",
    "    for page in range(1, total_pages + 1):\n",
    "        print(f\"Scraping page {page} of {total_pages}\")\n",
    "        \n",
    "        # change url with page number\n",
    "        url = f\"{base_url}&r={(page - 1) * 20 + 1}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        # same debugging step\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to retrieve data: {response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        cells = soup.find_all('td', align='left', height='10')\n",
    "        \n",
    "        \n",
    "        # from inspection in the url, tab-link contains the stock name\n",
    "        for cell in cells:\n",
    "            link = cell.find('a', class_='tab-link')\n",
    "            if link:\n",
    "                ticker = link.text\n",
    "                stocks.append(ticker)\n",
    "    \n",
    "    return stocks\n",
    "\n",
    "# Scrape the stock symbols\n",
    "stocks = scrape_finviz_stocks()\n",
    "\n",
    "if stocks:\n",
    "    # Save to a CSV file or use it directly\n",
    "    stocks_df = pd.DataFrame(stocks, columns=['Stock Symbol'])\n",
    "    stocks_df.to_csv('finviz_stocks.csv', index=False)\n",
    "    print(stocks)\n",
    "else:\n",
    "    print(\"No stocks found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2e24d59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_total_pages(soup):\n",
    "    \"\"\"Returns the total number of pages from the Finviz screener.\"\"\"\n",
    "    pages = soup.find_all('a', class_='screener-pages')\n",
    "    if pages:\n",
    "        # Get the text of the last page link\n",
    "        return int(pages[-2].text)\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
